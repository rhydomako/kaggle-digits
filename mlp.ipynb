{
 "metadata": {
  "name": "",
  "signature": "sha256:515822f7c08adeb14089a774897654dea850260e0738f7abc18ebb022b994293"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following the Deep Learning / Theano tutorial http://deeplearning.net/tutorial/logreg.html \n",
      "\n",
      "Multi-layer perceptron model for MNIST digit classification."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "import numpy as np\n",
      "import time\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib.pylab as plt\n",
      "plt.ioff()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_training_data():\n",
      "    \"\"\" \n",
      "        Load the labeled training examples that have already been formatted (format_data.py)\n",
      "        \n",
      "        return three pre-partitioned sets as theano shared arrays\n",
      "    \"\"\"\n",
      "    def shared_dataset(data):\n",
      "        inputs = theano.shared(np.asanyarray(data[0],dtype=theano.config.floatX))\n",
      "        target = theano.shared(np.asanyarray(data[1],dtype=theano.config.floatX))\n",
      "        return inputs, T.cast(target, 'int32')\n",
      "    \n",
      "    with open('digits.pkl', \"rb\") as f:\n",
      "        train, test, validation = cPickle.load(f)\n",
      "        \n",
      "    return [shared_dataset(train),\n",
      "            shared_dataset(test),\n",
      "            shared_dataset(validation)]\n",
      "\n",
      "def load_predict_data():\n",
      "    \"\"\"\n",
      "        Load the unlabeled data for Kaggle submission\n",
      "    \"\"\"\n",
      "    with open('predict.pkl', \"rb\") as f:\n",
      "        test_data = cPickle.load(f)\n",
      "    return test_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticRegression(object):\n",
      "    \"\"\"\n",
      "        The softmax function ammounts to a multinomial logistic regression model\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, inputs, n_in, n_out):\n",
      "        \n",
      "        #Weights matrix\n",
      "        self.W = theano.shared(\n",
      "            value=np.zeros(\n",
      "                (n_in, n_out),\n",
      "                dtype=theano.config.floatX\n",
      "            ),\n",
      "            name='W',\n",
      "            borrow=True\n",
      "        )\n",
      "        \n",
      "        #bias vector\n",
      "        self.b = theano.shared(\n",
      "            value=np.zeros(\n",
      "                (n_out,),\n",
      "                dtype=theano.config.floatX\n",
      "            ),\n",
      "            name='b',\n",
      "            borrow=True\n",
      "        )\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(inputs, self.W) + self.b)\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
      "        self.params = [self.W, self.b]\n",
      "        \n",
      "    def negative_log_likelihood(self, y):\n",
      "        \"\"\" Cost function \"\"\"\n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "\n",
      "    def errors(self, y):\n",
      "        \"\"\" Number of misclassified digits in a batch \"\"\"\n",
      "        return T.mean(T.neq(self.y_pred, y))\n",
      "    \n",
      "    \n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=T.tanh):\n",
      "        \n",
      "        self.input = input\n",
      "        \n",
      "        if W is None:\n",
      "            W_values = np.asarray(\n",
      "                rng.uniform(\n",
      "                    low=-np.sqrt(6. / (n_in + n_out)),\n",
      "                    high=np.sqrt(6. / (n_in + n_out)),\n",
      "                    size=(n_in, n_out)\n",
      "                ),\n",
      "                dtype=theano.config.floatX\n",
      "            )\n",
      "            if activation == theano.tensor.nnet.sigmoid:\n",
      "                W_values *= 4\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "        \n",
      "        \n",
      "class MLP(object):\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        self.hiddenLayer = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=T.tanh\n",
      "        )\n",
      "\n",
      "        # The logistic regression layer gets as input the hidden units\n",
      "        # of the hidden layer\n",
      "        self.logRegressionLayer = LogisticRegression(\n",
      "            inputs=self.hiddenLayer.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out\n",
      "        )\n",
      "\n",
      "        # L1 norm ; one regularization option is to enforce L1 norm to\n",
      "        # be small\n",
      "        self.L1 = (\n",
      "            abs(self.hiddenLayer.W).sum()\n",
      "            + abs(self.logRegressionLayer.W).sum()\n",
      "        )\n",
      "\n",
      "        # square of L2 norm ; one regularization option is to enforce\n",
      "        # square of L2 norm to be small\n",
      "        self.L2_sqr = (\n",
      "            (self.hiddenLayer.W ** 2).sum()\n",
      "            + (self.logRegressionLayer.W ** 2).sum()\n",
      "        )\n",
      "\n",
      "        # negative log likelihood of the MLP is given by the negative\n",
      "        # log likelihood of the output of the model, computed in the\n",
      "        # logistic regression layer\n",
      "        self.negative_log_likelihood = (\n",
      "            self.logRegressionLayer.negative_log_likelihood\n",
      "        )\n",
      "        # same holds for the function computing the number of errors\n",
      "        self.errors = self.logRegressionLayer.errors\n",
      "\n",
      "        # the parameters of the model are the parameters of the two layer it is\n",
      "        # made out of\n",
      "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load in the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train, test, validation = load_training_data()\n",
      "\n",
      "train_inputs, train_labels = train\n",
      "test_inputs , test_labels  = test\n",
      "validation_inputs, validation_labels = validation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Model parameters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "learning_rate=0.4\n",
      "n_epochs=1000\n",
      "batch_size=600\n",
      "n_hidden=500\n",
      "L1_reg=0.00\n",
      "L2_reg=0.0001\n",
      "\n",
      "train_batches = train_inputs.get_value().shape[0] / batch_size\n",
      "test_batches = test_inputs.get_value().shape[0] / batch_size\n",
      "validation_batches = validation_inputs.get_value().shape[0] / batch_size\n",
      "\n",
      "print \"Number of training batches: \" + str(train_batches) + \"\\n\" +\\\n",
      "      \"Number of test batches: \" + str(test_batches) + \"\\n\" +\\\n",
      "      \"Number of validation batches: \" + str(validation_batches)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start constructing the Theano computation graph.\n",
      "\n",
      "First, initialize the classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index = T.lscalar()\n",
      "\n",
      "inputs = T.matrix('x')\n",
      "labels = T.ivector('y')\n",
      "\n",
      "rng = np.random.RandomState(90210)\n",
      "\n",
      "# construct the MLP class\n",
      "classifier = MLP(\n",
      "    rng=rng,\n",
      "    input=inputs,\n",
      "    n_in=28 * 28,\n",
      "    n_hidden=n_hidden,\n",
      "    n_out=10\n",
      ")\n",
      "\n",
      "cost = (\n",
      "    classifier.negative_log_likelihood(labels)\n",
      "    + L1_reg * classifier.L1\n",
      "    + L2_reg * classifier.L2_sqr\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The test and validation models don't need to be trained, so they don't have the `update` parameters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_model = theano.function(\n",
      "    inputs=[index],\n",
      "    outputs=classifier.errors(labels),\n",
      "    givens={\n",
      "        inputs: test_inputs[index * batch_size: (index + 1) * batch_size],\n",
      "        labels: test_labels[index * batch_size: (index + 1) * batch_size]\n",
      "    }\n",
      ")\n",
      "\n",
      "validate_model = theano.function(\n",
      "    inputs=[index],\n",
      "    outputs=classifier.errors(labels),\n",
      "    givens={\n",
      "        inputs: validation_inputs[index * batch_size: (index + 1) * batch_size],\n",
      "        labels: validation_labels[index * batch_size: (index + 1) * batch_size]\n",
      "    }\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the training model, note the update parameter for the gradient descent."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gparams = [T.grad(cost, param) for param in classifier.params]\n",
      "\n",
      "updates = [\n",
      "    (param, param - learning_rate * gparam)\n",
      "    for param, gparam in zip(classifier.params, gparams)\n",
      "]\n",
      "\n",
      "train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            inputs: train_inputs[index * batch_size: (index + 1) * batch_size],\n",
      "            labels: train_labels[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parameters for the training loop:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "patience = 5000  # look as this many examples regardless\n",
      "patience_increase = 2  # wait this much longer when a new best is\n",
      "                                  # found\n",
      "improvement_threshold = 0.995  # a relative improvement of this much is\n",
      "                                  # considered significant\n",
      "validation_frequency = min(train_batches, patience / 2)\n",
      "                                  # go through this many\n",
      "                                  # minibatche before checking the network\n",
      "                                  # on the validation set; in this case we\n",
      "                                  # check every epoch\n",
      "best_validation_loss = np.inf\n",
      "test_score = 0."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training loop:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#initialize plotting\n",
      "plot_data=[pd.DataFrame(dict(epoch=-1,value=np.NaN,label='validation',unit='validation'),\n",
      "                        index=[0],\n",
      "                        columns=['epoch','value','label','unit']),\n",
      "           pd.DataFrame(dict(epoch=-1,value=np.NaN,label='test',unit='test'),\n",
      "                        index=[0],\n",
      "                        columns=['epoch','value','label','unit'])]\n",
      "plot_data=pd.concat(plot_data)\n",
      "\n",
      "\n",
      "start_time = time.clock()\n",
      "\n",
      "done_looping = False\n",
      "epoch = 0\n",
      "\n",
      "while (epoch < n_epochs) and (not done_looping):\n",
      "    epoch = epoch + 1\n",
      "    \n",
      "    test_value = np.NaN\n",
      "    validation_value = np.NaN\n",
      "\n",
      "    for minibatch_index in xrange(train_batches):\n",
      "        \n",
      "        minibatch_avg_cost = train_model(minibatch_index)\n",
      "        # iteration number\n",
      "        iter = (epoch - 1) * train_batches + minibatch_index\n",
      "                \n",
      "        if (iter + 1) % validation_frequency == 0:\n",
      "        # compute zero-one loss on validation set\n",
      "            validation_losses = [validate_model(i)\n",
      "                                 for i in xrange(validation_batches)]\n",
      "            this_validation_loss = np.mean(validation_losses)\n",
      "\n",
      "            print(\n",
      "                'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
      "                (\n",
      "                    epoch,\n",
      "                    minibatch_index + 1,\n",
      "                    train_batches,\n",
      "                    this_validation_loss * 100.\n",
      "                )\n",
      "            )\n",
      "            validation_value = this_validation_loss * 100.\n",
      "\n",
      "            # if we got the best validation score until now\n",
      "            if this_validation_loss < best_validation_loss:\n",
      "                #improve patience if loss improvement is good enough\n",
      "                if this_validation_loss < best_validation_loss *  \\\n",
      "                    improvement_threshold:\n",
      "                    patience = max(patience, iter * patience_increase)\n",
      "\n",
      "                best_validation_loss = this_validation_loss\n",
      "                # test it on the test set\n",
      "\n",
      "                test_losses = [test_model(i)\n",
      "                                for i in xrange(test_batches)]\n",
      "                test_score = np.mean(test_losses)\n",
      "\n",
      "                print(\n",
      "                    (\n",
      "                        '     epoch %i, minibatch %i/%i, test error of'\n",
      "                        ' best model %f %%'\n",
      "                    ) %\n",
      "                    (\n",
      "                        epoch,\n",
      "                        minibatch_index + 1,\n",
      "                        train_batches,\n",
      "                        test_score * 100.\n",
      "                    )\n",
      "                )\n",
      "                test_value = test_score * 100\n",
      "\n",
      "        if patience <= iter:\n",
      "            done_looping = True\n",
      "            break\n",
      "    \n",
      "    #plotting training curves\n",
      "    validation_values = pd.DataFrame(dict(epoch=epoch,value=validation_value,label='validation',unit='validation'),index=[0], columns=['epoch','value','label','unit'])\n",
      "    test_values       = pd.DataFrame(dict(epoch=epoch,value=test_value,label='test',unit='test'),index=[0], columns=['epoch','value','label','unit'])\n",
      "\n",
      "    plot_data = pd.concat([plot_data,validation_values,test_values])\n",
      "    sns.tsplot(plot_data,time='epoch',condition='label',value='value',unit='unit', interpolate=False, legend=False)\n",
      "    plt.savefig('curves.png')\n",
      "\n",
      "end_time = time.clock()\n",
      "\n",
      "print(\n",
      "        (\n",
      "            'Optimization complete with best validation score of %f %%,'\n",
      "            'with test performance %f %%'\n",
      "        )\n",
      "        % (best_validation_loss * 100., test_score * 100.)\n",
      "    )\n",
      "print 'The code run for %d epochs, with %f epochs/sec' % (\n",
      "    epoch, 1. * epoch / (end_time - start_time))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Output predictions for the unlabeled the Kaggle test data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predict = theano.function(\n",
      "    inputs=[inputs],\n",
      "    outputs=classifier.logRegressionLayer.y_pred,\n",
      "    allow_input_downcast=True\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Write predictions to a file for submission:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_data = load_predict_data()\n",
      "\n",
      "with open(\"mlp.\"+str(int(time.time())),\"w\") as f:\n",
      "    f.write(\"ImageId,Label\\n\")\n",
      "    ImageId = 1\n",
      "    for i in test_data[0]:\n",
      "        f.write( \",\".join([str(ImageId),str(predict(i.reshape(1,784))[0])]) + '\\n' )\n",
      "        ImageId += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}